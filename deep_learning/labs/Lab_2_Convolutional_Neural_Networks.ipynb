{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Convolutional Neural Networks\n",
    "\n",
    "In this lab we classify images from the [CIFAR-10 dataset](https://www.openml.org/d/40926) using convolutional neural networks in **PyTorch**.\n",
    "\n",
    "Tip: You can run these exercises faster on a GPU. If you don't have one locally, use Google Colab and enable GPU at *Runtime -> Change runtime type*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install --quiet openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import openml as oml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CIFAR-10 data\n",
    "cifar = oml.datasets.get_dataset(40926)\n",
    "X, y, _, _ = cifar.get_data(target=cifar.default_target_attribute, dataset_format='array')\n",
    "\n",
    "cifar_classes = {0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"cat\", 4: \"deer\",\n",
    "                 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"}\n",
    "\n",
    "# Reshape to (N, Channels, Height, Width) â€” PyTorch's expected format\n",
    "X = X.reshape((len(X), 3, 32, 32))\n",
    "y = y.astype(np.int64)\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some random examples\n",
    "from random import randint\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    n = randint(0, len(X) - 1)\n",
    "    axes[i].imshow(X[n].transpose(1, 2, 0).astype(np.uint8))  # CHW -> HWC for plotting\n",
    "    axes[i].set_xlabel(cifar_classes[int(y[n])])\n",
    "    axes[i].set_xticks(()); axes[i].set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: A simple model\n",
    "* Split the data into 80% training and 20% validation sets\n",
    "* Normalize the data to [0,1]\n",
    "* Build a ConvNet with 3 convolutional layers interspersed with `nn.MaxPool2d` layers, and one dense (`nn.Linear`) layer\n",
    "    * Use at least 32 3x3 filters in the first layer and ReLU activation\n",
    "    * Otherwise, make rational design choices or experiment a bit to see what works\n",
    "* You should at least get 60% accuracy\n",
    "* For training, you can try batch sizes of 64, and 20-50 epochs, but feel free to explore this as well\n",
    "* Plot and interpret the learning curves. Is the model overfitting? How could you improve it further?\n",
    "\n",
    "Hint: You can define models using `nn.Sequential(...)` and stack layers like `nn.Conv2d`, `nn.ReLU()`, `nn.MaxPool2d`, `nn.Flatten`, and `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: VGG-like model\n",
    "* Implement a simplified VGG model by building 3 'blocks' of 2 convolutional layers each\n",
    "* Do MaxPooling after each block\n",
    "* The first block should use at least 32 filters, later blocks should use more\n",
    "* Use 3x3 filters with `padding=1` to preserve spatial dimensions within each block\n",
    "* Use a dense layer with at least 128 hidden nodes\n",
    "* Use ReLU activations everywhere (where it makes sense)\n",
    "* Plot and interpret the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Regularization\n",
    "* Explore different ways to regularize your VGG-like model\n",
    "  * Try adding `nn.Dropout(p)` after every MaxPooling and Dense layer\n",
    "    * What are good Dropout rates? Try a fixed rate, or increase rates in the deeper layers\n",
    "  * Try `nn.BatchNorm2d` together with Dropout\n",
    "    * Think about where batch normalization would make sense\n",
    "* Plot and interpret the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Data Augmentation\n",
    "* Perform image augmentation using `torchvision.transforms` (e.g. `RandomHorizontalFlip`, `RandomAffine`, `RandomCrop`)\n",
    "* You will need a custom Dataset class that applies transforms in `__getitem__`\n",
    "* What is the effect? What is the effect with and without Dropout?\n",
    "* Plot and interpret the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Interpret the misclassifications\n",
    "Chances are that even your best model is not yet perfect. It is important to understand what kind of errors it still makes.\n",
    "* Run the validation images through the network and detect all misclassified ones\n",
    "* Visualize some of the misclassifications. Are they to be expected?\n",
    "* Compute the confusion matrix (e.g. using `sklearn.metrics.confusion_matrix`). Which classes are often confused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Interpret the model\n",
    "Retrain your best model on all the data. Then, visualize the activations (feature maps) for a sample image at each convolutional layer.\n",
    "\n",
    "Hint: In PyTorch, you can use `register_forward_hook` on a layer to capture its output during a forward pass.\n",
    "\n",
    "Interpret the results. Is your model learning something useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Take it a step further\n",
    "* Repeat the exercises with a [higher-resolution version](https://www.openml.org/d/41103) (OpenML ID 41103), or a [version with 100 classes](https://www.openml.org/d/41983) (OpenML ID 41983)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
