{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Neural Networks\n",
    "\n",
    "In this lab we build dense neural networks on the MNIST dataset using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install --quiet openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import openml as oml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST\n",
    "mnist = oml.datasets.get_dataset(554)\n",
    "X, y, _, _ = mnist.get_data(target=mnist.default_target_attribute, dataset_format='array')\n",
    "X = X.reshape(70000, 28, 28)\n",
    "\n",
    "# Visualize some examples\n",
    "from random import randint\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    n = randint(0, len(X) - 1)\n",
    "    axes[i].imshow(X[n], cmap=plt.cm.gray_r)\n",
    "    axes[i].set_xlabel(y[n])\n",
    "    axes[i].set_xticks(()); axes[i].set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined 60000/10000 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=60000, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Preprocessing\n",
    "* Normalize the data: map pixel values from 0-255 to 0.0-1.0\n",
    "* Flatten the data from (N, 28, 28) to (N, 784)\n",
    "* Convert the numpy arrays to PyTorch tensors (`torch.float32` for X, `torch.long` for y)\n",
    "* Create a `TensorDataset` for training and another for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Create a neural network model\n",
    "\n",
    "Implement a `create_model` function that builds a dense neural network using `nn.Sequential`:\n",
    "* 2 layers: one hidden layer and one output layer\n",
    "* The number of nodes in each layer should be function parameters\n",
    "* Add at least one `nn.Dropout` layer for regularization\n",
    "\n",
    "Consider:\n",
    "* Input size: 28x28 = 784 (flattened)\n",
    "* Hidden layer: use `nn.ReLU()` activation\n",
    "* Output: 10 classes â€” no softmax needed since `nn.CrossEntropyLoss` handles it internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layer_1_units=32, layer_2_units=10, dropout_rate=0.3):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Create a training function\n",
    "\n",
    "Implement a `train_model` function that:\n",
    "* Creates `DataLoader`s from the datasets\n",
    "* Uses `nn.CrossEntropyLoss` and `torch.optim.Adam`\n",
    "* Runs a training loop for a given number of epochs\n",
    "* Computes and prints train/validation loss and accuracy each epoch\n",
    "* Returns a history dict with `loss`, `accuracy`, `val_loss`, `val_accuracy` lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, epochs=10, batch_size=64, learning_rate=0.001):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Evaluate the model\n",
    "\n",
    "* Train the model with: learning rate 0.003, 50 epochs, batch size 4000, and a 20% validation split\n",
    "* Plot the learning curves (loss and accuracy for train and validation)\n",
    "* Report performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper plotting function\n",
    "def plot_curve(history):\n",
    "    epochs = range(1, len(history[\"accuracy\"]) + 1)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax1.plot(epochs, history[\"accuracy\"], label=\"Train\")\n",
    "    ax1.plot(epochs, history[\"val_accuracy\"], label=\"Val\")\n",
    "    ax1.set_title(\"Accuracy\"); ax1.legend()\n",
    "    ax2.plot(epochs, history[\"loss\"], label=\"Train\")\n",
    "    ax2.plot(epochs, history[\"val_loss\"], label=\"Val\")\n",
    "    ax2.set_title(\"Loss\"); ax2.legend()\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Optimize the model\n",
    "\n",
    "Try to optimize the model to reach at least **96% test accuracy**. Experiment with:\n",
    "* The number of hidden layers\n",
    "* The number of neurons per layer\n",
    "* Dropout rates\n",
    "* Learning rate and batch size\n",
    "\n",
    "Hint: A model with layers like [256, 128, 64] and dropout 0.2 should get there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
