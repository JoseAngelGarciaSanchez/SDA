{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Transformers\n",
    "\n",
    "In this lab we apply the Transformer architecture to two tasks:\n",
    "1. **Sequence-to-Sequence** modelling (reversing a sequence)\n",
    "2. **Set Anomaly Detection** (finding the odd image out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "DATASET_PATH = os.environ.get(\"DATASET_PATH\", \"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Sequence to Sequence\n",
    "\n",
    "Given a sequence of $N$ numbers between $0$ and $M$, the task is to **reverse** the input sequence. In NumPy notation, if our input is $x$, the output should be $x$[::-1].\n",
    "\n",
    "We'll use only a Transformer encoder for this simple task.\n",
    "\n",
    "### The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = partial(ReverseDataset, 10, 16)\n",
    "train_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader   = data.DataLoader(dataset(1000),  batch_size=128)\n",
    "test_loader  = data.DataLoader(dataset(10000), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a sample\n",
    "inp_data, labels = train_loader.dataset[0]\n",
    "print(\"Input data:\", inp_data)\n",
    "print(\"Labels:    \", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Build the model\n",
    "\n",
    "Create a `ReversePredictor` model:\n",
    "- Use `nn.Embedding` to convert each input number into an embedding vector\n",
    "- Add **positional encoding** so the model knows about the order of the sequence\n",
    "- Pass through one or more `nn.TransformerEncoderLayer` blocks\n",
    "- Predict the output for each position using a linear layer on top\n",
    "- Use Cross-Entropy loss\n",
    "\n",
    "Hint: `nn.TransformerEncoder` wraps multiple `nn.TransformerEncoderLayer` blocks. Start with 1 layer and 1 attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Train the model\n",
    "\n",
    "Write a training loop and train the model. Tips:\n",
    "- A single encoder block and single attention head should be enough\n",
    "- Try gradient clipping (`torch.nn.utils.clip_grad_norm_`) to stabilize training\n",
    "- Test your model on the test set — it should reach near-perfect accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Visualize the attention\n",
    "\n",
    "Visualize the attention weights from the Multi-Head Attention block for an arbitrary input.\n",
    "\n",
    "Hint: You can pass `need_weights=True` to the attention layer, or use hooks to capture them. The attention pattern should show that position $i$ attends to position $N-1-i$ (the reversed position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Set Anomaly Detection\n",
    "\n",
    "Transformers are well-suited for **set** problems because Multi-Head Attention is permutation-equivariant.\n",
    "\n",
    "**Task:** Given a set of 10 images where 9 belong to the same class and 1 does not, identify the anomaly.\n",
    "\n",
    "We use CIFAR-100 (100 classes, 600 images each at 32x32). We first extract features using a pre-trained ResNet34, then feed these features into a Transformer.\n",
    "\n",
    "### The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet normalization statistics\n",
    "DATA_MEANS = np.array([0.485, 0.456, 0.406])\n",
    "DATA_STD = np.array([0.229, 0.224, 0.225])\n",
    "TORCH_DATA_MEANS = torch.from_numpy(DATA_MEANS).view(1, 3, 1, 1)\n",
    "TORCH_DATA_STD = torch.from_numpy(DATA_STD).view(1, 3, 1, 1)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "])\n",
    "\n",
    "train_set = CIFAR100(root=DATASET_PATH, train=True,  transform=transform, download=True)\n",
    "test_set  = CIFAR100(root=DATASET_PATH, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Extract features with a pre-trained ResNet\n",
    "\n",
    "Load a pre-trained ResNet34 from `torchvision.models` and extract features (the output before the final classification layer) for all images. Store these features so you don't need to recompute them.\n",
    "\n",
    "Hint: Remove the last FC layer or use a hook to capture the features from the penultimate layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up train/val/test splits and the anomaly dataset\n",
    "\n",
    "We split the training data 90/10 into train/val in a balanced way, then create the anomaly detection datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into train + val (90/10 balanced)\n",
    "labels = torch.LongTensor(train_set.targets)\n",
    "num_labels = labels.max() + 1\n",
    "sorted_indices = torch.argsort(labels).reshape(num_labels, -1)\n",
    "\n",
    "num_val_exmps = sorted_indices.shape[1] // 10\n",
    "val_indices   = sorted_indices[:, :num_val_exmps].reshape(-1)\n",
    "train_indices = sorted_indices[:, num_val_exmps:].reshape(-1)\n",
    "\n",
    "# Replace train_set_feats below with your extracted features\n",
    "# train_feats, train_labels = train_set_feats[train_indices], labels[train_indices]\n",
    "# val_feats,   val_labels   = train_set_feats[val_indices],   labels[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAnomalyDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, img_feats, labels, set_size=10, train=True):\n",
    "        \"\"\"\n",
    "        img_feats - Tensor [num_imgs, feat_dim]: high-level features from ResNet\n",
    "        labels    - Tensor [num_imgs]: class labels\n",
    "        set_size  - Number of elements in a set (N-1 same class + 1 anomaly)\n",
    "        train     - If True, sample a new set each time __getitem__ is called\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_feats = img_feats\n",
    "        self.labels = labels\n",
    "        self.set_size = set_size - 1\n",
    "        self.train = train\n",
    "\n",
    "        self.num_labels = labels.max() + 1\n",
    "        self.img_idx_by_label = torch.argsort(self.labels).reshape(self.num_labels, -1)\n",
    "\n",
    "        if not train:\n",
    "            self.test_sets = self._create_test_sets()\n",
    "\n",
    "    def _create_test_sets(self):\n",
    "        np.random.seed(42)\n",
    "        test_sets = [self.sample_img_set(self.labels[idx]) for idx in range(len(self.img_feats))]\n",
    "        return torch.stack(test_sets, dim=0)\n",
    "\n",
    "    def sample_img_set(self, anomaly_label):\n",
    "        set_label = np.random.randint(self.num_labels - 1)\n",
    "        if set_label >= anomaly_label:\n",
    "            set_label += 1\n",
    "        img_indices = np.random.choice(self.img_idx_by_label.shape[1], size=self.set_size, replace=False)\n",
    "        img_indices = self.img_idx_by_label[set_label, img_indices]\n",
    "        return img_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.img_feats.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anomaly = self.img_feats[idx]\n",
    "        if self.train:\n",
    "            img_indices = self.sample_img_set(self.labels[idx])\n",
    "        else:\n",
    "            img_indices = self.test_sets[idx]\n",
    "        # Anomaly is always the last image\n",
    "        img_set = torch.cat([self.img_feats[img_indices], anomaly[None]], dim=0)\n",
    "        indices = torch.cat([img_indices, torch.LongTensor([idx])], dim=0)\n",
    "        label = img_set.shape[0] - 1\n",
    "        return img_set, indices, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders (uncomment after extracting features)\n",
    "SET_SIZE = 10\n",
    "test_labels = torch.LongTensor(test_set.targets)\n",
    "\n",
    "# train_anom_dataset = SetAnomalyDataset(train_feats, train_labels, set_size=SET_SIZE, train=True)\n",
    "# val_anom_dataset   = SetAnomalyDataset(val_feats,   val_labels,   set_size=SET_SIZE, train=False)\n",
    "# test_anom_dataset  = SetAnomalyDataset(test_feats,  test_labels,  set_size=SET_SIZE, train=False)\n",
    "\n",
    "# train_anom_loader = data.DataLoader(train_anom_dataset, batch_size=64, shuffle=True,  drop_last=True, pin_memory=True)\n",
    "# val_anom_loader   = data.DataLoader(val_anom_dataset,   batch_size=64, shuffle=False)\n",
    "# test_anom_loader  = data.DataLoader(test_anom_dataset,  batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_exmp(indices, orig_dataset):\n",
    "    images = [orig_dataset[idx][0] for idx in indices.reshape(-1)]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    images = images * TORCH_DATA_STD + TORCH_DATA_MEANS\n",
    "    img_grid = torchvision.utils.make_grid(images, nrow=SET_SIZE, normalize=True, pad_value=0.5, padding=16)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Anomaly examples on CIFAR100 (last image = anomaly)\")\n",
    "    plt.imshow(img_grid.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment after creating test_anom_loader:\n",
    "# _, indices, _ = next(iter(test_anom_loader))\n",
    "# visualize_exmp(indices[:4], test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Build the anomaly detection model\n",
    "\n",
    "Write a Transformer-based model that takes a **set** of image features and outputs one logit per image. Apply softmax over these logits and train the anomaly image to have the highest probability.\n",
    "\n",
    "Since the prediction must be permutation-equivariant, the Transformer is a natural choice. The input to each element is a ResNet feature vector (512-dim for ResNet34)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Train the model\n",
    "\n",
    "Write a training loop (same structure as for the reverse task). Train your model and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Visualize the attention\n",
    "\n",
    "Plot the images in the input set, the model's prediction, and the attention maps from different heads/layers. This helps interpret what information the model shares between images.\n",
    "\n",
    "Explore different input examples — are there cases where the task is harder (e.g. visually similar classes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
